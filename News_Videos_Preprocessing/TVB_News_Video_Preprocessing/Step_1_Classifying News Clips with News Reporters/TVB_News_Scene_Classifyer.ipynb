{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b817d3",
   "metadata": {},
   "source": [
    "# Generating thumbnails for each News Video Clips by extracting the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c042c09-b638-45c4-ba86-9885e0978b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150319/150319 [1:28:15<00:00, 28.39it/s]  \n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "input_folder = \"./TVB Scraped Videos\"\n",
    "output_folder = \"./thumbnails\"\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_video(file_name):\n",
    "    if file_name.endswith(\".mp4\"):  \n",
    "        video_path = os.path.join(input_folder, file_name)\n",
    "        output_file = os.path.join(output_folder, file_name.replace(\".mp4\", \".jpg\"))\n",
    "\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        success, frame = cap.read()  \n",
    "\n",
    "        if success:\n",
    "            \n",
    "            cv2.imwrite(output_file, frame)\n",
    "            return f\"Saved first frame of {file_name} as {output_file}\"\n",
    "        else:\n",
    "            return f\"Failed to read video: {file_name}\"\n",
    "\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(input_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(tqdm(executor.map(process_video, files), total=len(files)))\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4c720",
   "metadata": {},
   "source": [
    "# Generating Features for the first frame by using OpenAI's CLIP Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200d587-4e0b-4d4b-9a5c-ccd283b53c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 150316/150316 [1:19:47<00:00, 31.40image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              feature_0  feature_1  feature_2  feature_3  \\\n",
      "image_name                                                                 \n",
      "59654a72e6038331360802e0.jpg   0.792969   0.591797  -0.376465   0.232056   \n",
      "59654fede60383a6480802e0.jpg   0.248413   0.289551   0.216919   0.069458   \n",
      "596556a2c5e16c4e33bc8475.jpg   0.489990   0.417480   0.080505   0.453125   \n",
      "59655ff4e603831f360802e1.jpg   0.698242   0.916992   0.349121  -0.351318   \n",
      "59656058c5e16c6152bc8474.jpg   0.030075   0.958984   0.365967   0.928711   \n",
      "\n",
      "                              feature_4  feature_5  feature_6  feature_7  \\\n",
      "image_name                                                                 \n",
      "59654a72e6038331360802e0.jpg  -0.377441  -0.150879  -0.351318   0.724121   \n",
      "59654fede60383a6480802e0.jpg  -0.516113   0.211914   0.143677   0.669922   \n",
      "596556a2c5e16c4e33bc8475.jpg  -0.350342  -0.025803   0.140381   0.560547   \n",
      "59655ff4e603831f360802e1.jpg  -0.388184  -0.139893   0.382568   0.516602   \n",
      "59656058c5e16c6152bc8474.jpg  -0.677734   0.564941   0.382568   0.213989   \n",
      "\n",
      "                              feature_8  feature_9  ...  feature_758  \\\n",
      "image_name                                          ...                \n",
      "59654a72e6038331360802e0.jpg   0.711914  -0.025070  ...    -0.115479   \n",
      "59654fede60383a6480802e0.jpg  -0.108459  -0.511719  ...     0.122803   \n",
      "596556a2c5e16c4e33bc8475.jpg  -0.206909  -0.468994  ...     0.203613   \n",
      "59655ff4e603831f360802e1.jpg   0.652832  -0.122070  ...     0.419922   \n",
      "59656058c5e16c6152bc8474.jpg   0.522461   0.119507  ...    -0.006569   \n",
      "\n",
      "                              feature_759  feature_760  feature_761  \\\n",
      "image_name                                                            \n",
      "59654a72e6038331360802e0.jpg    -0.495850     0.415527     0.233276   \n",
      "59654fede60383a6480802e0.jpg    -0.800781     0.082092    -0.295654   \n",
      "596556a2c5e16c4e33bc8475.jpg    -0.712402     0.117676    -0.320068   \n",
      "59655ff4e603831f360802e1.jpg    -0.321045     0.259277    -0.317871   \n",
      "59656058c5e16c6152bc8474.jpg    -0.258057     0.433594     0.102417   \n",
      "\n",
      "                              feature_762  feature_763  feature_764  \\\n",
      "image_name                                                            \n",
      "59654a72e6038331360802e0.jpg    -0.086487     0.407959    -0.305176   \n",
      "59654fede60383a6480802e0.jpg    -0.180176     1.010742     0.109436   \n",
      "596556a2c5e16c4e33bc8475.jpg    -0.253662     1.085938     0.204956   \n",
      "59655ff4e603831f360802e1.jpg    -0.053894     0.523926    -0.012207   \n",
      "59656058c5e16c6152bc8474.jpg    -0.209961     0.469238    -0.169189   \n",
      "\n",
      "                              feature_765  feature_766  feature_767  \n",
      "image_name                                                           \n",
      "59654a72e6038331360802e0.jpg    -0.396484     0.214722    -0.524902  \n",
      "59654fede60383a6480802e0.jpg    -0.001283     0.818848     0.293213  \n",
      "596556a2c5e16c4e33bc8475.jpg    -0.075317     1.048828     0.251953  \n",
      "59655ff4e603831f360802e1.jpg    -0.308350     0.048309    -0.116943  \n",
      "59656058c5e16c6152bc8474.jpg    -0.339111     0.277832    -0.958496  \n",
      "\n",
      "[5 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm  \n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)  \n",
    "\n",
    "\n",
    "def load_images_and_extract_features(folder, model, preprocess, device):\n",
    "    image_names = []\n",
    "    features = []\n",
    "    file_list = [f for f in os.listdir(folder) if f.endswith(('.png', '.jpg', '.jpeg'))]  \n",
    "\n",
    "    \n",
    "    for file_name in tqdm(file_list, desc=\"Processing images\", unit=\"image\"):\n",
    "        image_path = os.path.join(folder, file_name)\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model.encode_image(image).cpu().numpy().flatten()\n",
    "        image_names.append(file_name)\n",
    "        features.append(feature)\n",
    "    \n",
    "    return pd.DataFrame(features, index=image_names, columns=[f\"feature_{i}\" for i in range(768)])\n",
    "\n",
    "\n",
    "image_folder = \"./TVB_clip_feature_extraction/thumbnails\"  \n",
    "\n",
    "\n",
    "df_features = load_images_and_extract_features(image_folder, model, preprocess, device)\n",
    "df_features.index.name = \"image_name\"  \n",
    "print(df_features.head())  \n",
    "\n",
    "\n",
    "df_features.to_pickle(\"tvb_clip_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52c109",
   "metadata": {},
   "source": [
    "# Manually Labelling 5,000 Thumbnails as \"News Reporter Presented\" and \"News Reporter NOT Presented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tkinter import Tk, Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "\n",
    "image_folder = \"./TVB_clip_feature_extraction/thumbnails\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "\n",
    "label_file = \"tvb_labels.csv\"\n",
    "if os.path.exists(label_file):\n",
    "    df = pd.read_csv(label_file, index_col=0)  \n",
    "else:\n",
    "    df = pd.DataFrame({\"image_name\": image_files, \"is_reporter\": None})  \n",
    "\n",
    "\n",
    "\n",
    "unlabeled_indices = df[df[\"is_reporter\"].isna()].index.tolist()\n",
    "current_index = unlabeled_indices[0] if unlabeled_indices else -1  \n",
    "\n",
    "\n",
    "class LabelingApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Labeling Tool\")\n",
    "\n",
    "        \n",
    "        self.current_index = current_index\n",
    "        self.df = df\n",
    "\n",
    "        \n",
    "        self.image_label = Label(root)\n",
    "        self.image_label.pack()\n",
    "\n",
    "        \n",
    "        button_font = (\"Helvetica\", 16, \"bold\")  \n",
    "        self.true_button = Button(root, text=\"True\", command=self.mark_true, width=30, height=2, bg=\"lightgreen\", font=button_font)\n",
    "        self.true_button.pack(side=\"left\", padx=20, pady=20)\n",
    "\n",
    "        self.false_button = Button(root, text=\"False\", command=self.mark_false, width=30, height=2, bg=\"lightcoral\", font=button_font)\n",
    "        self.false_button.pack(side=\"right\", padx=20, pady=20)\n",
    "\n",
    "        \n",
    "        self.root.bind(\"<j>\", lambda event: self.mark_true())\n",
    "        self.root.bind(\"<k>\", lambda event: self.mark_false())\n",
    "        self.root.bind(\"<h>\", lambda event: self.go_back())  \n",
    "\n",
    "        \n",
    "        self.update_image()\n",
    "\n",
    "    def update_image(self):\n",
    "        \n",
    "        if self.current_index == -1:\n",
    "            self.image_label.config(text=\"All images have been labeled!\")\n",
    "            self.true_button.config(state=\"disabled\")\n",
    "            self.false_button.config(state=\"disabled\")\n",
    "            return\n",
    "\n",
    "        \n",
    "        img_path = os.path.join(image_folder, self.df.iloc[self.current_index][\"image_name\"])\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        \n",
    "        img = img.resize((854, 480))\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "\n",
    "        \n",
    "        self.image_label.config(image=img)\n",
    "        self.image_label.image = img\n",
    "        self.root.title(f\"Image {self.current_index + 1}/{len(self.df)}\")\n",
    "\n",
    "    def mark_true(self):\n",
    "        self.df.loc[self.current_index, \"is_reporter\"] = True\n",
    "        self.save_and_next()\n",
    "\n",
    "    def mark_false(self):\n",
    "        self.df.loc[self.current_index, \"is_reporter\"] = False\n",
    "        self.save_and_next()\n",
    "\n",
    "    def go_back(self):\n",
    "        \n",
    "        if self.current_index > 0:\n",
    "            self.current_index -= 1  \n",
    "            self.df.loc[self.current_index, \"is_reporter\"] = None  \n",
    "            self.update_image()\n",
    "\n",
    "    def save_and_next(self):\n",
    "        \n",
    "        self.df.to_csv(label_file)\n",
    "\n",
    "        \n",
    "        unlabeled_indices = self.df[self.df[\"is_reporter\"].isna()].index.tolist()\n",
    "        self.current_index = unlabeled_indices[0] if unlabeled_indices else -1\n",
    "\n",
    "        \n",
    "        self.update_image()\n",
    "\n",
    "\n",
    "root = Tk()\n",
    "app = LabelingApp(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ee4cd",
   "metadata": {},
   "source": [
    "# Using a Simple Neural Network to Classify the videos as \"News Reporter Presented\" and \"News Reporter NOT Presented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57792a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6068f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_features = pd.read_pickle(\"tvb_clip_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b36908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              feature_0  feature_1  feature_2  feature_3  \\\n",
      "image_name                                                                 \n",
      "59654a72e6038331360802e0.jpg   0.792969   0.591797  -0.376465   0.232056   \n",
      "\n",
      "                              feature_4  feature_5  feature_6  feature_7  \\\n",
      "image_name                                                                 \n",
      "59654a72e6038331360802e0.jpg  -0.377441  -0.150879  -0.351318   0.724121   \n",
      "\n",
      "                              feature_8  feature_9  ...  feature_758  \\\n",
      "image_name                                          ...                \n",
      "59654a72e6038331360802e0.jpg   0.711914   -0.02507  ...    -0.115479   \n",
      "\n",
      "                              feature_759  feature_760  feature_761  \\\n",
      "image_name                                                            \n",
      "59654a72e6038331360802e0.jpg     -0.49585     0.415527     0.233276   \n",
      "\n",
      "                              feature_762  feature_763  feature_764  \\\n",
      "image_name                                                            \n",
      "59654a72e6038331360802e0.jpg    -0.086487     0.407959    -0.305176   \n",
      "\n",
      "                              feature_765  feature_766  feature_767  \n",
      "image_name                                                           \n",
      "59654a72e6038331360802e0.jpg    -0.396484     0.214722    -0.524902  \n",
      "\n",
      "[1 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "print(clip_features.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48209497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.0726\n",
      "Epoch 2/100, Loss: 0.0071\n",
      "Epoch 3/100, Loss: 0.0033\n",
      "Epoch 4/100, Loss: 0.0014\n",
      "Epoch 5/100, Loss: 0.0008\n",
      "Epoch 6/100, Loss: 0.0006\n",
      "Epoch 7/100, Loss: 0.0005\n",
      "Epoch 8/100, Loss: 0.0004\n",
      "Epoch 9/100, Loss: 0.0003\n",
      "Epoch 10/100, Loss: 0.0002\n",
      "Epoch 11/100, Loss: 0.0002\n",
      "Epoch 12/100, Loss: 0.0001\n",
      "Epoch 13/100, Loss: 0.0001\n",
      "Epoch 14/100, Loss: 0.0001\n",
      "Epoch 15/100, Loss: 0.0001\n",
      "Epoch 16/100, Loss: 0.0001\n",
      "Epoch 17/100, Loss: 0.0001\n",
      "Epoch 18/100, Loss: 0.0001\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0001\n",
      "Epoch 21/100, Loss: 0.0000\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0000\n",
      "Epoch 24/100, Loss: 0.0001\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       125\n",
      "         1.0       1.00      1.00      1.00       481\n",
      "\n",
      "    accuracy                           1.00       606\n",
      "   macro avg       1.00      1.00      1.00       606\n",
      "weighted avg       1.00      1.00      1.00       606\n",
      "\n",
      "Accuracy: 0.9983\n",
      "Model saved as complex_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, output_dim=1, dropout_rate=0.5):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"tvb_labels.csv\", index_col=0)\n",
    "\n",
    "\n",
    "features_df = clip_features\n",
    "\n",
    "\n",
    "data = labels_df.merge(features_df, on=\"image_name\")\n",
    "\n",
    "\n",
    "data = data.dropna(subset=[\"is_reporter\"])\n",
    "\n",
    "\n",
    "X = data[[f\"feature_{i}\" for i in range(768)]].values\n",
    "y = data[\"is_reporter\"].astype(int).values  \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "input_dim = 768\n",
    "model = ComplexClassifier(input_dim=input_dim, hidden_dim=512, output_dim=1, dropout_rate=0.5)\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        y_true.extend(targets.numpy())\n",
    "        y_pred.extend(predictions.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"complex_classifier.pth\")\n",
    "print(\"Model saved as complex_classifier.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bd982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels saved to labels_predicted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"tvb_labels.csv\", index_col=0)\n",
    "features_df = clip_features\n",
    "\n",
    "\n",
    "data = labels_df.merge(features_df, on=\"image_name\", how=\"left\")\n",
    "\n",
    "\n",
    "labeled_data = data.dropna(subset=[\"is_reporter\"])  \n",
    "unlabeled_data = data[data[\"is_reporter\"].isna()]  \n",
    "\n",
    "\n",
    "class ComplexClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, output_dim=1, dropout_rate=0.5):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "input_dim = 768\n",
    "model = ComplexClassifier(input_dim=input_dim, hidden_dim=512, output_dim=1, dropout_rate=0.5)\n",
    "model.load_state_dict(torch.load(\"complex_classifier.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "if not unlabeled_data.empty:\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_labeled = labeled_data[[f\"feature_{i}\" for i in range(768)]]\n",
    "    X_unlabeled = unlabeled_data[[f\"feature_{i}\" for i in range(768)]]\n",
    "\n",
    "    \n",
    "    scaler.fit(X_labeled)  \n",
    "    X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "\n",
    "    \n",
    "    X_unlabeled_tensor = torch.tensor(X_unlabeled_scaled, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_unlabeled_tensor).squeeze()\n",
    "        predictions = (predictions > 0.5)  \n",
    "\n",
    "    \n",
    "    unlabeled_data = unlabeled_data.copy()  \n",
    "    unlabeled_data.loc[:, \"is_reporter\"] = predictions.numpy()\n",
    "\n",
    "\n",
    "labels_predicted_df = pd.concat([labeled_data, unlabeled_data]).sort_index()\n",
    "\n",
    "\n",
    "labels_predicted_df = labels_predicted_df[[\"image_name\", \"is_reporter\"]]\n",
    "\n",
    "\n",
    "labels_predicted_df.to_csv(\"labels_predicted.csv\", index=False)\n",
    "print(\"Predicted labels saved to labels_predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9d2ee-5021-4d0d-9a4b-b2fa6002a1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
