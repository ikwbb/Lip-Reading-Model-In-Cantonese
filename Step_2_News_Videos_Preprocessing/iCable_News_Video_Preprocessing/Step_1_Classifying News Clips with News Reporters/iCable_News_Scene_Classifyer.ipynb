{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da60c42b",
   "metadata": {},
   "source": [
    "# Extracting the First Frame of each Video as the Thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 37843/37843 [26:01<00:00, 24.23image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "image_name                                                                     \n",
      "0.jpg        0.378174   0.776367  -0.139771  -0.042175  -0.418457  -0.729004   \n",
      "1.jpg        0.219116   0.103516   0.297363   2.056641   0.383057  -0.051331   \n",
      "100.jpg      0.366211   0.254639   0.304443   0.751465  -0.732910   0.181152   \n",
      "1000.jpg     0.377441   1.109375   0.409668   0.067505  -0.665527  -0.292236   \n",
      "10000.jpg    0.506836  -0.531250   0.203979   0.172363  -0.760742   1.029297   \n",
      "\n",
      "            feature_6  feature_7  feature_8  feature_9  ...  feature_758  \\\n",
      "image_name                                              ...                \n",
      "0.jpg        0.275879  -0.557617   0.355957  -0.012924  ...     0.018524   \n",
      "1.jpg       -0.192505   0.036285   0.273926   0.054596  ...     0.379395   \n",
      "100.jpg     -0.967285   0.604004   0.493896  -0.218628  ...     0.587402   \n",
      "1000.jpg     0.194336  -0.181641   0.719727   0.189697  ...    -0.041565   \n",
      "10000.jpg   -0.091125   1.071289  -0.158691  -0.559082  ...    -0.382568   \n",
      "\n",
      "            feature_759  feature_760  feature_761  feature_762  feature_763  \\\n",
      "image_name                                                                    \n",
      "0.jpg         -0.908691    -0.226929     0.220825    -0.154175     0.348389   \n",
      "1.jpg         -0.399658     0.043121     0.305176     0.350342     0.865723   \n",
      "100.jpg       -0.770508    -0.346436    -0.197998    -1.182617     0.229614   \n",
      "1000.jpg      -0.570312    -0.377441     0.338867    -0.077576     0.148682   \n",
      "10000.jpg     -0.147339    -0.264893    -0.075317    -0.957520     0.375732   \n",
      "\n",
      "            feature_764  feature_765  feature_766  feature_767  \n",
      "image_name                                                      \n",
      "0.jpg         -0.486816     0.259521     0.489746     0.989746  \n",
      "1.jpg         -0.040009     0.634277     0.393799     0.114075  \n",
      "100.jpg        0.449707     0.645996    -0.934570    -0.067383  \n",
      "1000.jpg      -0.431641     0.186646     0.545410     0.723145  \n",
      "10000.jpg      0.085449    -0.022446     0.640137    -0.186523  \n",
      "\n",
      "[5 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm  \n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)  \n",
    "\n",
    "\n",
    "def load_images_and_extract_features(folder, model, preprocess, device):\n",
    "    image_names = []\n",
    "    features = []\n",
    "    file_list = [f for f in os.listdir(folder) if f.endswith(('.png', '.jpg', '.jpeg'))]  \n",
    "\n",
    "    \n",
    "    for file_name in tqdm(file_list, desc=\"Processing images\", unit=\"image\"):\n",
    "        image_path = os.path.join(folder, file_name)\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model.encode_image(image).cpu().numpy().flatten()\n",
    "        image_names.append(file_name)\n",
    "        features.append(feature)\n",
    "    \n",
    "    return pd.DataFrame(features, index=image_names, columns=[f\"feature_{i}\" for i in range(768)])\n",
    "\n",
    "\n",
    "image_folder = \"./icable-news-scraping/thumbnails_analysis/thumbnails\"  \n",
    "\n",
    "\n",
    "df_features = load_images_and_extract_features(image_folder, model, preprocess, device)\n",
    "df_features.index.name = \"image_name\"  \n",
    "print(df_features.head())  \n",
    "\n",
    "\n",
    "df_features.to_pickle(\"clip_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfd359",
   "metadata": {},
   "source": [
    "# Manually Labelling 2,000 Thumbnails as \"News Reporter Presented\" and \"News Reporter NOT Presented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tkinter import Tk, Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "\n",
    "image_folder = \"./icable-news-scraping/thumbnails_analysis/thumbnails\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "\n",
    "label_file = \"labels.csv\"\n",
    "if os.path.exists(label_file):\n",
    "    df = pd.read_csv(label_file, index_col=0)  \n",
    "else:\n",
    "    df = pd.DataFrame({\"image_name\": image_files, \"is_reporter\": None})  \n",
    "\n",
    "\n",
    "\n",
    "unlabeled_indices = df[df[\"is_reporter\"].isna()].index.tolist()\n",
    "current_index = unlabeled_indices[0] if unlabeled_indices else -1  \n",
    "\n",
    "\n",
    "class LabelingApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Labeling Tool\")\n",
    "\n",
    "        \n",
    "        self.current_index = current_index\n",
    "        self.df = df\n",
    "\n",
    "        \n",
    "        self.image_label = Label(root)\n",
    "        self.image_label.pack()\n",
    "\n",
    "        \n",
    "        button_font = (\"Helvetica\", 16, \"bold\")  \n",
    "        self.true_button = Button(root, text=\"True\", command=self.mark_true, width=30, height=2, bg=\"lightgreen\", font=button_font)\n",
    "        self.true_button.pack(side=\"left\", padx=20, pady=20)\n",
    "\n",
    "        self.false_button = Button(root, text=\"False\", command=self.mark_false, width=30, height=2, bg=\"lightcoral\", font=button_font)\n",
    "        self.false_button.pack(side=\"right\", padx=20, pady=20)\n",
    "\n",
    "        \n",
    "        self.root.bind(\"<j>\", lambda event: self.mark_true())\n",
    "        self.root.bind(\"<k>\", lambda event: self.mark_false())\n",
    "        self.root.bind(\"<h>\", lambda event: self.go_back())  \n",
    "\n",
    "        \n",
    "        self.update_image()\n",
    "\n",
    "    def update_image(self):\n",
    "        \n",
    "        if self.current_index == -1:\n",
    "            self.image_label.config(text=\"All images have been labeled!\")\n",
    "            self.true_button.config(state=\"disabled\")\n",
    "            self.false_button.config(state=\"disabled\")\n",
    "            return\n",
    "\n",
    "        \n",
    "        img_path = os.path.join(image_folder, self.df.iloc[self.current_index][\"image_name\"])\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        \n",
    "        img = img.resize((854, 480))\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "\n",
    "        \n",
    "        self.image_label.config(image=img)\n",
    "        self.image_label.image = img\n",
    "        self.root.title(f\"Image {self.current_index + 1}/{len(self.df)}\")\n",
    "\n",
    "    def mark_true(self):\n",
    "        self.df.loc[self.current_index, \"is_reporter\"] = True\n",
    "        self.save_and_next()\n",
    "\n",
    "    def mark_false(self):\n",
    "        self.df.loc[self.current_index, \"is_reporter\"] = False\n",
    "        self.save_and_next()\n",
    "\n",
    "    def go_back(self):\n",
    "        \n",
    "        if self.current_index > 0:\n",
    "            self.current_index -= 1  \n",
    "            self.df.loc[self.current_index, \"is_reporter\"] = None  \n",
    "            self.update_image()\n",
    "\n",
    "    def save_and_next(self):\n",
    "        \n",
    "        self.df.to_csv(label_file)\n",
    "\n",
    "        \n",
    "        unlabeled_indices = self.df[self.df[\"is_reporter\"].isna()].index.tolist()\n",
    "        self.current_index = unlabeled_indices[0] if unlabeled_indices else -1\n",
    "\n",
    "        \n",
    "        self.update_image()\n",
    "\n",
    "\n",
    "root = Tk()\n",
    "app = LabelingApp(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd9c79",
   "metadata": {},
   "source": [
    "# Using a Simple Neural Network to Classify the videos as \"News Reporter Presented\" and \"News Reporter NOT Presented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_features = pd.read_pickle(\"clip_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5a694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "image_name                                                                     \n",
      "0.jpg        0.378174   0.776367  -0.139771  -0.042175  -0.418457  -0.729004   \n",
      "\n",
      "            feature_6  feature_7  feature_8  feature_9  ...  feature_758  \\\n",
      "image_name                                              ...                \n",
      "0.jpg        0.275879  -0.557617   0.355957  -0.012924  ...     0.018524   \n",
      "\n",
      "            feature_759  feature_760  feature_761  feature_762  feature_763  \\\n",
      "image_name                                                                    \n",
      "0.jpg         -0.908691    -0.226929     0.220825    -0.154175     0.348389   \n",
      "\n",
      "            feature_764  feature_765  feature_766  feature_767  \n",
      "image_name                                                      \n",
      "0.jpg         -0.486816     0.259521     0.489746     0.989746  \n",
      "\n",
      "[1 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "print(clip_features.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.1613\n",
      "Epoch 2/100, Loss: 0.0444\n",
      "Epoch 3/100, Loss: 0.0228\n",
      "Epoch 4/100, Loss: 0.0129\n",
      "Epoch 5/100, Loss: 0.0075\n",
      "Epoch 6/100, Loss: 0.0063\n",
      "Epoch 7/100, Loss: 0.0038\n",
      "Epoch 8/100, Loss: 0.0026\n",
      "Epoch 9/100, Loss: 0.0028\n",
      "Epoch 10/100, Loss: 0.0019\n",
      "Epoch 11/100, Loss: 0.0017\n",
      "Epoch 12/100, Loss: 0.0009\n",
      "Epoch 13/100, Loss: 0.0008\n",
      "Epoch 14/100, Loss: 0.0007\n",
      "Epoch 15/100, Loss: 0.0006\n",
      "Epoch 16/100, Loss: 0.0008\n",
      "Epoch 17/100, Loss: 0.0007\n",
      "Epoch 18/100, Loss: 0.0006\n",
      "Epoch 19/100, Loss: 0.0005\n",
      "Epoch 20/100, Loss: 0.0004\n",
      "Epoch 21/100, Loss: 0.0004\n",
      "Epoch 22/100, Loss: 0.0004\n",
      "Epoch 23/100, Loss: 0.0002\n",
      "Epoch 24/100, Loss: 0.0002\n",
      "Epoch 25/100, Loss: 0.0002\n",
      "Epoch 26/100, Loss: 0.0002\n",
      "Epoch 27/100, Loss: 0.0002\n",
      "Epoch 28/100, Loss: 0.0002\n",
      "Epoch 29/100, Loss: 0.0002\n",
      "Epoch 30/100, Loss: 0.0001\n",
      "Epoch 31/100, Loss: 0.0001\n",
      "Epoch 32/100, Loss: 0.0002\n",
      "Epoch 33/100, Loss: 0.0001\n",
      "Epoch 34/100, Loss: 0.0001\n",
      "Epoch 35/100, Loss: 0.0001\n",
      "Epoch 36/100, Loss: 0.0002\n",
      "Epoch 37/100, Loss: 0.0001\n",
      "Epoch 38/100, Loss: 0.0001\n",
      "Epoch 39/100, Loss: 0.0001\n",
      "Epoch 40/100, Loss: 0.0001\n",
      "Epoch 41/100, Loss: 0.0001\n",
      "Epoch 42/100, Loss: 0.0001\n",
      "Epoch 43/100, Loss: 0.0001\n",
      "Epoch 44/100, Loss: 0.0001\n",
      "Epoch 45/100, Loss: 0.0001\n",
      "Epoch 46/100, Loss: 0.0001\n",
      "Epoch 47/100, Loss: 0.0001\n",
      "Epoch 48/100, Loss: 0.0001\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0001\n",
      "Epoch 51/100, Loss: 0.0001\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0001\n",
      "Epoch 54/100, Loss: 0.0001\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0001\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0001\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96        57\n",
      "         1.0       0.99      0.99      0.99       349\n",
      "\n",
      "    accuracy                           0.99       406\n",
      "   macro avg       0.97      0.98      0.97       406\n",
      "weighted avg       0.99      0.99      0.99       406\n",
      "\n",
      "Accuracy: 0.9877\n",
      "Model saved as complex_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, output_dim=1, dropout_rate=0.5):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"labels.csv\", index_col=0)\n",
    "\n",
    "\n",
    "features_df = clip_features\n",
    "\n",
    "\n",
    "data = labels_df.merge(features_df, on=\"image_name\")\n",
    "\n",
    "\n",
    "data = data.dropna(subset=[\"is_reporter\"])\n",
    "\n",
    "\n",
    "X = data[[f\"feature_{i}\" for i in range(768)]].values\n",
    "y = data[\"is_reporter\"].astype(int).values  \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "input_dim = 768\n",
    "model = ComplexClassifier(input_dim=input_dim, hidden_dim=512, output_dim=1, dropout_rate=0.5)\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        y_true.extend(targets.numpy())\n",
    "        y_pred.extend(predictions.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"complex_classifier.pth\")\n",
    "print(\"Model saved as complex_classifier.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
